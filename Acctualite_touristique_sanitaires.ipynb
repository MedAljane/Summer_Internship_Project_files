{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Acctualite touristique_sanitaires.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MedAljane/Summer_Internship_Project_files/blob/main/Acctualite_touristique_sanitaires.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBUCj8n1T8GO"
      },
      "source": [
        "!pip install sentence-splitter\n",
        "!pip install transformers\n",
        "!pip install SentencePiece\n",
        "!pip install cdifflib\n",
        "!pip install deep-translator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U71dEjudqrnR"
      },
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import urllib.request\n",
        "import json\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "from deep_translator import GoogleTranslator\n",
        "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
        "import datetime\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5oYphvzqrpz"
      },
      "source": [
        "# https://huggingface.co/tuner007/pegasus_paraphrase\n",
        "\n",
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "\n",
        "# Cette fonction importe un modèle pré-entraîné à partir d'un dépôt GitHub créé par \"tuner007\"\n",
        "# Ainsi le script doit s'exécuter sur un GPU pour des performances accélérées\n",
        "\n",
        "model_name = 'tuner007/pegasus_paraphrase'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "\n",
        "def get_response(input_text,num_return_sequences):\n",
        "  batch = tokenizer.prepare_seq2seq_batch([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
        "  translated = model.generate(**batch,max_length=60,num_beams=10, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "  return tgt_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN1flVOgxM_F"
      },
      "source": [
        "# TransPrasing Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXKKfq7txMm8"
      },
      "source": [
        "def transPhrasing(text):\n",
        "    splitter = SentenceSplitter(language='fr')\n",
        "    sentence_list = splitter.split(text)\n",
        "\n",
        "    translated_sentences = [GoogleTranslator(target='en').translate(element) for element in sentence_list]\n",
        "    paraphrased_sentences = [get_response(e,1) for e in translated_sentences]\n",
        "    cleaned_paraphrased_sentences = [e[0] for e in paraphrased_sentences]\n",
        "    sentences_to_french = [GoogleTranslator(target='fr').translate(e) for e in cleaned_paraphrased_sentences]\n",
        "    paraphrased_result = [' '.join(x for x in sentences_to_french)]\n",
        "    paraphrased_result_cleaned = str(paraphrased_result).strip('[]').strip(\"'\").strip('\"')\n",
        "\n",
        "    return paraphrased_result_cleaned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY3RG-CAwrkt"
      },
      "source": [
        "# Departement Paris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L59RNz-Jwurg"
      },
      "source": [
        "## Actualites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgYKhNHCwoo-"
      },
      "source": [
        "req = requests.get('https://actu.fr/ile-de-france/paris_75056')\n",
        "soup = BeautifulSoup(req.content , 'html.parser')\n",
        "\n",
        "links = []\n",
        "artPrevLarge = soup.find('article',class_='ac-preview-article ac-preview-article-large-v').find('a')['href']\n",
        "links.append(artPrevLarge)\n",
        "\n",
        "artPrevSmall= soup.findAll('li',class_='ac-preview-article ac-preview-article-small')\n",
        "for e in artPrevSmall:\n",
        "    links.append(e.find('a')['href'])\n",
        "\n",
        "artPrevMedium= soup.findAll('li',class_='ac-preview-article ac-preview-article-medium')\n",
        "for e in artPrevMedium:\n",
        "    links.append(e.find('a')['href'])\n",
        "Publish_Dates= []\n",
        "Articles_Content = []\n",
        "liensActualiteParis = []\n",
        "imagesActualiteParis = []\n",
        "titreActualiteParis = []\n",
        "\n",
        "l=links[0:6]\n",
        "for link in l:\n",
        "    r=requests.get(link)\n",
        "    soup=BeautifulSoup(r.content , 'html.parser')\n",
        "\n",
        "        #Get the link of the article\n",
        "    liensActualiteParis.append(link)\n",
        "\n",
        "        #Get the title of the article\n",
        "    titreActualiteParis.append(transPhrasing(soup.find('div', {'class': 'ac-article-main '}).find('h1').text.strip()))\n",
        "\n",
        "        #Get the preview image of the article\n",
        "    imagesActualiteParis.append(soup.find('article', {'class': 'js-article-inner'}).find('img')['src'])\n",
        "\n",
        "        #Get the paragraphs just under the div\n",
        "    t = soup.find('div', class_='ac-article-content')\n",
        "    children = t.findChildren(\"p\" , recursive=False)\n",
        "    art_content= \"\"\n",
        "    for e in children: \n",
        "        if e.name == 'p':\n",
        "            art_content += e.text\n",
        "    art_content = art_content.replace(u'\\xa0', u' ')\n",
        "\n",
        "        #Get the publishing date\n",
        "    pub_date = GoogleTranslator(target='fr').translate(datetime.datetime.fromisoformat(soup.find('div', class_='ac-article-date').find('time')['datetime']).strftime(\"%B %d, %Y at %H:%M\"))\n",
        "    \n",
        "    Publish_Dates.append(pub_date)\n",
        "    Articles_Content.append(art_content)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVBpg4xMwouE"
      },
      "source": [
        "paris_tou=[]\n",
        "for i in Articles_Content:\n",
        "  text = i\n",
        "  splitter = SentenceSplitter(language='fr')\n",
        "  sentence_list = splitter.split(text)\n",
        "\n",
        "  translated_sentences = [GoogleTranslator(target='en').translate(element) for element in sentence_list]\n",
        "  paraphrased_sentences = [get_response(e,1) for e in translated_sentences]\n",
        "  cleaned_paraphrased_sentences = [e[0] for e in paraphrased_sentences]\n",
        "  sentences_to_french = [GoogleTranslator(target='fr').translate(e) for e in cleaned_paraphrased_sentences]\n",
        "  paraphrased_result = [' '.join(x for x in sentences_to_french)]\n",
        "  paraphrased_result_cleaned = str(paraphrased_result).strip('[]').strip(\"'\").strip('\"')\n",
        "  paris_tou.append(paraphrased_result_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7tR-_SIwowZ"
      },
      "source": [
        "Articles = {\n",
        "        'dates': Publish_Dates,\n",
        "        'image article': imagesActualiteParis,\n",
        "        'actualites' : paris_tou,\n",
        "        'liensActualite': liensActualiteParis,\n",
        "        'titreActualite': titreActualiteParis\n",
        "    } \n",
        "paris_dft = pd.DataFrame(Articles)\n",
        "paris_dft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aogCap7kwoyp"
      },
      "source": [
        "paris_t=paris_dft.transpose()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYyTLp-Gwo1Q"
      },
      "source": [
        "paris_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiLyStx_Xy-2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAzYWGyEXzBN"
      },
      "source": [
        "#département de paris ( Coronavirus )\n",
        "p=requests.get('https://www.sortiraparis.com/actualites/coronavirus')\n",
        "soup=BeautifulSoup(p.content , 'html.parser')\n",
        "liens=[]\n",
        "for i in soup.findAll('div',{'class':'item'}):\n",
        "    l=i.find('a')\n",
        "    liens.append(\"https://www.sortiraparis.com\"+l['href'])\n",
        "article1=[]\n",
        "article2=[]\n",
        "dates_cor=[]\n",
        "for li in liens :\n",
        "  p=requests.get(li)\n",
        "  soup=BeautifulSoup(p.content , 'html.parser')\n",
        "  for i in soup.findAll('div',{'class':'published'}):\n",
        "    dates_cor.append(i.text)\n",
        "  try :\n",
        "    for i in soup.findAll('div',{'class':'abstract'})[0]:\n",
        "      article1.append(i)\n",
        "  except:\n",
        "    pass\n",
        "  for j in soup.findAll('div',{'class':'article'}):\n",
        "    article2.append(j.text)\n",
        "test1=[]\n",
        "for i in range(len(article1)):\n",
        "  test1.append(article1[i])\n",
        "liste1=[]\n",
        "for i in test1 :\n",
        "  i = re.sub(r'\\[[0-9]*\\]', ' ', i)\n",
        "  i = re.sub(r'\\s+', ' ', i)\n",
        "  liste1.append(i)\n",
        "\n",
        "test2=[]\n",
        "for i in range(len(article2)):\n",
        "  test2.append(article2[i])\n",
        "liste2=[]\n",
        "for i in test2 :\n",
        "  i = re.sub(r'\\[[0-9]*\\]', ' ', i)\n",
        "  i = re.sub(r'\\s+', ' ', i)\n",
        "  liste2.append(i)\n",
        "list_cor = ['{} {}'.format(x,y) for x,y in zip(liste1,liste2)]\n",
        "d=[]\n",
        "for i in dates_cor :\n",
        "  i = re.sub(r'\\[[0-9]*\\]', ' ', i)\n",
        "  i = re.sub(r'\\s+', ' ', i)\n",
        "  d.append(i)\n",
        "txt2=\"\"\n",
        "for i in d :\n",
        "    txt2 += i \n",
        "    t2=txt2.split('Publié')\n",
        "liste=[]\n",
        "for i in t2[1:len(t2)] :\n",
        "  liste.append(i)\n",
        "date_c=[]\n",
        "for i in liste:\n",
        "  date_c.append(i[0:60])\n",
        "\n",
        "\n",
        "#Recuperations des images\n",
        "imagesActualiteDepParis = ['https://www.sortiraparis.com'+BeautifulSoup(requests.get(li).content , 'html.parser').find('div', {'id': 'article-carousel'}).find('a')['href'] for li in liens[:6]]\n",
        "liensActualiteDepParis = [li for li in liens[:6]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GtoPHw7YQNj"
      },
      "source": [
        "paris_cor=[]\n",
        "for i in list_cor[:6]:\n",
        "  text = i\n",
        "  splitter = SentenceSplitter(language='fr')\n",
        "  sentence_list = splitter.split(text)\n",
        "\n",
        "  translated_sentences = [GoogleTranslator(target='en').translate(element) for element in sentence_list]\n",
        "  paraphrased_sentences = [get_response(e,1) for e in translated_sentences]\n",
        "  cleaned_paraphrased_sentences = [e[0] for e in paraphrased_sentences]\n",
        "  sentences_to_french = [GoogleTranslator(target='fr').translate(e) for e in cleaned_paraphrased_sentences]\n",
        "  paraphrased_result = [' '.join(x for x in sentences_to_french)]\n",
        "  paraphrased_result_cleaned = str(paraphrased_result).strip('[]').strip(\"'\").strip('\"')\n",
        "  paris_cor.append(paraphrased_result_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDskYDWNVxWX"
      },
      "source": [
        "dict = {'date': date_c[:6],\n",
        "        'image actualite': imagesActualiteDepParis[:6],\n",
        "        'actualite' : paris_cor[:6],\n",
        "        'liensActualite': liensActualiteDepParis\n",
        "        } \n",
        "paris_df1 = pd.DataFrame(dict)\n",
        "paris_df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbzSFF5rVxY-"
      },
      "source": [
        "paris_c=paris_df1.transpose()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KsvM6AoTUgJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnbFhyniTUi3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plR-pUXOTUlj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlCfQmgwUhOL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03l3MegfcJJX"
      },
      "source": [
        "# seine-et-marne\n",
        "p=requests.get('https://www.seine-et-marne.fr/fr/actualites')\n",
        "soup=BeautifulSoup(p.content , 'html.parser')\n",
        "liens1=[]\n",
        "dates=[]\n",
        "\n",
        "for i in soup.findAll('article',{'class':'ribbon-item link-context__context -news node--promoted node--view-mode-teaser'}):\n",
        "  liens1.append('https://www.seine-et-marne.fr'+i.a['href'])\n",
        "for i in soup.findAll('span',{'class':'date__item'}):\n",
        "  dates.append(i.text)\n",
        "artic=[]\n",
        "for i in liens1 :\n",
        "  p=requests.get(i)\n",
        "  soup=BeautifulSoup(p.content , 'html.parser')\n",
        "  for i in soup.findAll('section',{'class':'site-content'}):\n",
        "    artic.append(i.text)\n",
        "test=[]\n",
        "for i in range(len(artic)):\n",
        "  test.append(artic[i])\n",
        "liste=[]\n",
        "for i in test :\n",
        "  i = re.sub(r'\\[[0-9]*\\]', ' ', i)\n",
        "  i = re.sub(r'\\s+', ' ', i)\n",
        "  liste.append(i)\n",
        "dd=dates[0:6]\n",
        "ls=liste[0:6]\n",
        "#Recuperation images des articles:\n",
        "imagesActualiteSeM = [BeautifulSoup(requests.get(li).content, 'html.parser').find('div', {'class': 'heading__group'}).find('img')['src'] for li in liens1[:6]]\n",
        "liensActualiteSeM = [li for li in liens1[:6]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSPWC3lScJPI"
      },
      "source": [
        "seine_tour=[]\n",
        "for i in ls:\n",
        "  text = i\n",
        "  splitter = SentenceSplitter(language='fr')\n",
        "  sentence_list = splitter.split(text)\n",
        "\n",
        "  translated_sentences = [GoogleTranslator(target='en').translate(element) for element in sentence_list]\n",
        "  paraphrased_sentences = [get_response(e,1) for e in translated_sentences]\n",
        "  cleaned_paraphrased_sentences = [e[0] for e in paraphrased_sentences]\n",
        "  sentences_to_french = [GoogleTranslator(target='fr').translate(e) for e in cleaned_paraphrased_sentences]\n",
        "  paraphrased_result = [' '.join(x for x in sentences_to_french)]\n",
        "  paraphrased_result_cleaned = str(paraphrased_result).strip('[]').strip(\"'\").strip('\"')\n",
        "  seine_tour.append(paraphrased_result_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wVcZR1IdI2Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaFaCXQcUhQh"
      },
      "source": [
        "dict = {'date': dd,\n",
        "        'image actualite': imagesActualiteSeM,\n",
        "        'acctualite' : seine_tour,\n",
        "        'liensActualite': liensActualiteSeM\n",
        "        } \n",
        "seine_df1 = pd.DataFrame(dict)\n",
        "seine_df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS-wlQnre4WV"
      },
      "source": [
        "seine_t=seine_df1.transpose()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nx6SOkfe4Yu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OA1_45Ue4bb"
      },
      "source": [
        "# seine-et-marne (coronavirus)\n",
        "\n",
        "req_SeM = requests.get('https://actu.fr/la-republique-de-seine-et-marne/societe/coronavirus')\n",
        "soup_SeM = BeautifulSoup(req_SeM.content , 'html.parser')\n",
        "\n",
        "links_SeM = []\n",
        "artPrevLarge_SeM = soup_SeM.find('article',class_='ac-preview-article ac-preview-article-large-v').find('a')['href']\n",
        "links_SeM.append(artPrevLarge_SeM)\n",
        "\n",
        "artPrevSmall_SeM = soup_SeM.findAll('li',class_='ac-preview-article ac-preview-article-small')\n",
        "for e in artPrevSmall_SeM:\n",
        "    links_SeM.append(e.find('a')['href'])\n",
        "\n",
        "artPrevMedium_SeM= soup_SeM.findAll('li',class_='ac-preview-article ac-preview-article-medium')\n",
        "for e in artPrevMedium_SeM:\n",
        "    links_SeM.append(e.find('a')['href'])\n",
        "\n",
        "Publish_Dates_SeM = []\n",
        "Articles_Content_SeM = []\n",
        "ls=links_SeM[0:6]\n",
        "for link in ls:\n",
        "    r=requests.get(link)\n",
        "    soup_SeM=BeautifulSoup(r.content , 'html.parser')\n",
        "    t = soup_SeM.find('div', class_='ac-article-content')\n",
        "\n",
        "    children = t.findChildren(\"p\" , recursive=False)\n",
        "    art_content_SeM = \"\"\n",
        "    for e in children:\n",
        "        if e.name == 'p':\n",
        "            art_content_SeM += e.text\n",
        "    art_content_SeM = art_content_SeM.replace(u'\\xa0', u' ')\n",
        "\n",
        "    pub_date_SeM = soup_SeM.find('div', class_='ac-article-date').find('time')['datetime']\n",
        "    \n",
        "    Publish_Dates_SeM.append(pub_date_SeM)\n",
        "    Articles_Content_SeM.append(art_content_SeM)\n",
        "\n",
        "#Recuperation images des articles:\n",
        "imagesActualiteSeMCor = [BeautifulSoup(requests.get(li).content, 'html.parser').find('article', {'class': 'js-article-inner'}).find('img')['src'] for li in ls]\n",
        "liensActualiteSeMCor = [li for li in ls]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eopk0pJcasfr"
      },
      "source": [
        "seine_cor=[]\n",
        "for i in Articles_Content_SeM:\n",
        "  text = i\n",
        "  splitter = SentenceSplitter(language='fr')\n",
        "  sentence_list = splitter.split(text)\n",
        "\n",
        "  translated_sentences = [GoogleTranslator(target='en').translate(element) for element in sentence_list]\n",
        "  paraphrased_sentences = [get_response(e,1) for e in translated_sentences]\n",
        "  cleaned_paraphrased_sentences = [e[0] for e in paraphrased_sentences]\n",
        "  sentences_to_french = [GoogleTranslator(target='fr').translate(e) for e in cleaned_paraphrased_sentences]\n",
        "  paraphrased_result = [' '.join(x for x in sentences_to_french)]\n",
        "  paraphrased_result_cleaned = str(paraphrased_result).strip('[]').strip(\"'\").strip('\"')\n",
        "  seine_cor.append(paraphrased_result_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TJDuS7Me4d5"
      },
      "source": [
        "date_sen=[]\n",
        "try:\n",
        "  for i in Publish_Dates_SeM:\n",
        "    date_sen.append(i.replace('T','  '))\n",
        "except:\n",
        "  date_sen=Publish_Dates_SeM\n",
        "date_s=[ r.strip().strip().split('+')[0] for r in date_sen ]\n",
        "Articles_SeM = {'date': date_s,\n",
        "                'image actualite': imagesActualiteSeMCor,\n",
        "                'actualite' : seine_cor,\n",
        "                'liensActualite': liensActualiteSeMCor\n",
        "        } \n",
        "SeM_df = pd.DataFrame(Articles_SeM)\n",
        "SeM_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb69YEpWvdKS"
      },
      "source": [
        "seine_c=SeM_df.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJiZzHeVbXFr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQQkao5RbXIB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLi1bL8to5J0"
      },
      "source": [
        "#département de yvelines \n",
        "req = requests.get('https://actu.fr/78actu/')\n",
        "soup = BeautifulSoup(req.content , 'html.parser')\n",
        "\n",
        "links_yveline = []\n",
        "artPrevLarge_yveline = soup.find('article',class_='ac-preview-article ac-preview-article-large-v').find('a')['href']\n",
        "links_yveline.append(artPrevLarge_yveline)\n",
        "\n",
        "artPrevSmall_yveline = soup.findAll('li',class_='ac-preview-article ac-preview-article-small')\n",
        "for e in artPrevSmall_yveline:\n",
        "    links_yveline.append(e.find('a')['href'])\n",
        "\n",
        "artPrevMedium_yveline= soup.findAll('li',class_='ac-preview-article ac-preview-article-medium')\n",
        "for e in artPrevMedium_yveline:\n",
        "    links_yveline.append(e.find('a')['href'])\n",
        "Publish_Dates_yveline = []\n",
        "Articles_Content_yveline = []\n",
        "links=links_yveline[0:6]\n",
        "for link in links:\n",
        "    r=requests.get(link)\n",
        "    soup=BeautifulSoup(r.content , 'html.parser')\n",
        "    t = soup.find('div', class_='ac-article-content')\n",
        "\n",
        "    #Get the paragraphs just under the div\n",
        "    children = t.findChildren(\"p\" , recursive=False)\n",
        "    art_content_yveline= \"\"\n",
        "    for e in children:\n",
        "        if e.name == 'p':\n",
        "            art_content_yveline += e.text\n",
        "    art_content_yveline = art_content_yveline.replace(u'\\xa0', u' ')\n",
        "\n",
        "    #Get the publishing time\n",
        "    pub_date_yveline= soup.find('div', class_='ac-article-date').find('time')['datetime']\n",
        "    \n",
        "    Publish_Dates_yveline.append(pub_date_yveline)\n",
        "    Articles_Content_yveline.append(art_content_yveline)\n",
        "    \n",
        "dates=[]\n",
        "try:\n",
        "  for i in Publish_Dates_yveline:\n",
        "    dates.append(i.replace('T','  '))\n",
        "except:\n",
        "  dates=Publish_Dates_yveline\n",
        "date_y=[ r.strip().strip().split('+')[0] for r in dates ]\n",
        "\n",
        "#Recuperation images des articles:\n",
        "imagesActualiteYveline = [BeautifulSoup(requests.get(li).content, 'html.parser').find('article', {'class': 'js-article-inner'}).find('img')['src'] for li in links_yveline[:6]]\n",
        "liensActualiteYveline = [li for li in links_yveline[:6]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCU39Dv6pMTQ"
      },
      "source": [
        "yveline_tou=[]\n",
        "for i in Articles_Content_yveline:\n",
        "  text = i\n",
        "  splitter = SentenceSplitter(language='fr')\n",
        "  sentence_list = splitter.split(text)\n",
        "\n",
        "  translated_sentences = [GoogleTranslator(target='en').translate(element) for element in sentence_list]\n",
        "  paraphrased_sentences = [get_response(e,1) for e in translated_sentences]\n",
        "  cleaned_paraphrased_sentences = [e[0] for e in paraphrased_sentences]\n",
        "  sentences_to_french = [GoogleTranslator(target='fr').translate(e) for e in cleaned_paraphrased_sentences]\n",
        "  paraphrased_result = [' '.join(x for x in sentences_to_french)]\n",
        "  paraphrased_result_cleaned = str(paraphrased_result).strip('[]').strip(\"'\").strip('\"')\n",
        "  yveline_tou.append(paraphrased_result_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi-tm2xdo5MZ"
      },
      "source": [
        "Articles_yveline= {\n",
        "        'dates': date_y,\n",
        "        'image actualite': imagesActualiteYveline,\n",
        "        'actualites' : yveline_tou,\n",
        "        'liensActualite': liensActualiteYveline\n",
        "        } \n",
        "yveline_dft = pd.DataFrame(Articles_yveline)\n",
        "yveline_dft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWdb-gS-o5Ox"
      },
      "source": [
        "yveline_t=yveline_dft.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3412ZUSHeuX_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vuBJCTSqxda"
      },
      "source": [
        "### département de yvelines (coronavirus)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLrAwmkefGUA"
      },
      "source": [
        "# Recuperation de liens des articles\n",
        "req = requests.get('https://actu.fr/78actu/societe/coronavirus')\n",
        "soup = BeautifulSoup(req.content , 'html.parser')\n",
        "\n",
        "links_yveline = []\n",
        "artPrevLarge_yveline = soup.find('article',class_='ac-preview-article ac-preview-article-large-v').find('a')['href']\n",
        "links_yveline.append(artPrevLarge_yveline)\n",
        "\n",
        "artPrevSmall_yveline = soup.findAll('li',class_='ac-preview-article ac-preview-article-small')\n",
        "for e in artPrevSmall_yveline:\n",
        "    links_yveline.append(e.find('a')['href'])\n",
        "\n",
        "artPrevMedium_yveline= soup.findAll('li',class_='ac-preview-article ac-preview-article-medium')\n",
        "for e in artPrevMedium_yveline:\n",
        "    links_yveline.append(e.find('a')['href'])\n",
        "\n",
        "# Recuperation de contenus des articles\n",
        "Publish_Dates_yveline = []\n",
        "Articles_Content_yveline = []\n",
        "\n",
        "for link in links_yveline[0:6]:\n",
        "    r=requests.get(link)\n",
        "    soup=BeautifulSoup(r.content , 'html.parser')\n",
        "    t = soup.find('div', class_='ac-article-content')\n",
        "\n",
        "    #Get the paragraphs just under the div\n",
        "    children = t.findChildren(\"p\" , recursive=False)\n",
        "    art_content_yveline= \"\"\n",
        "    for e in children:\n",
        "        if e.name == 'p':\n",
        "            art_content_yveline += e.text\n",
        "    art_content_yveline = art_content_yveline.replace(u'\\xa0', u' ')\n",
        "\n",
        "    #Get the publishing time\n",
        "    pub_date_yveline= soup.find('div', class_='ac-article-date').find('time')['datetime']\n",
        "    \n",
        "    Publish_Dates_yveline.append(pub_date_yveline)\n",
        "    Articles_Content_yveline.append(art_content_yveline)\n",
        "\n",
        "date_yc = [datetime.datetime.fromisoformat(x).strftime(\"%B %d, %Y at %H:%M\") for x in Publish_Dates_yveline]\n",
        "\n",
        "#Recuperation images des articles:\n",
        "imagesActualiteYvelineCor = [BeautifulSoup(requests.get(li).content, 'html.parser').find('article', {'class': 'js-article-inner'}).find('img')['src'] for li in links_yveline[:6]]\n",
        "liensActualiteYvelineCor = [li for li in links_yveline[:6]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2oMopwQkkah"
      },
      "source": [
        "yveline_cor=[]\n",
        "for i in Articles_Content_yveline:\n",
        "  text = i\n",
        "  splitter = SentenceSplitter(language='fr')\n",
        "  sentence_list = splitter.split(text)\n",
        "\n",
        "  translated_sentences = [GoogleTranslator(target='en').translate(element) for element in sentence_list]\n",
        "  paraphrased_sentences = [get_response(e,1) for e in translated_sentences]\n",
        "  cleaned_paraphrased_sentences = [e[0] for e in paraphrased_sentences]\n",
        "  sentences_to_french = [GoogleTranslator(target='fr').translate(e) for e in cleaned_paraphrased_sentences]\n",
        "  paraphrased_result = [' '.join(x for x in sentences_to_french)]\n",
        "  paraphrased_result_cleaned = str(paraphrased_result).strip('[]').strip(\"'\").strip('\"')\n",
        "  yveline_cor.append(paraphrased_result_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzWqczuJfGWj"
      },
      "source": [
        "Articles_yveline= {\n",
        "        'dates': date_yc,\n",
        "        'image actualite': imagesActualiteYvelineCor,\n",
        "        'actualites' : yveline_cor,\n",
        "        'liensActualite': liensActualiteYvelineCor\n",
        "        } \n",
        "yveline_df = pd.DataFrame(Articles_yveline)\n",
        "yveline_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g4MvQg3hzTD"
      },
      "source": [
        "yveline_c=yveline_df.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEtDWetFfGbt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_djdo7h7b0DQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owIqHtGrb0Ft"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e_7Thnseuac"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2sgDKK52pC4"
      },
      "source": [
        "### Département de Oise "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JXhtBvAisAb"
      },
      "source": [
        "\n",
        "liens=[]\n",
        "ds=[]\n",
        "link='https://france3-regions.francetvinfo.fr/hauts-de-france/oise'\n",
        "p=requests.get(link)\n",
        "soup=BeautifulSoup(p.content , 'html.parser')\n",
        "for i in soup.findAll('div',{'class':'article-card__date'}):\n",
        "    ds.append(i['data-date'])\n",
        "for i in soup.findAll('a',{'class':'article-card__title'}):\n",
        "    liens.append(i['href'])\n",
        "li1=[]\n",
        "li2=[]\n",
        "lif=[]\n",
        "for i in  range(len(liens)) :\n",
        "  if  'coronavirus'  not in liens[i]:\n",
        "    li1.append(liens[i])\n",
        "for i in  range(len(li1)) :\n",
        "  if  'covid'  not in li1[i]:\n",
        "    li2.append(li1[i])\n",
        "for i in  range(len(li2)) :\n",
        "  if  'vaccination'  not in li2[i]:\n",
        "    lif.append(li2[i])\n",
        "act=[]\n",
        "lo=lif[0:6]\n",
        "for i in lo :\n",
        "    p=requests.get(i)\n",
        "    soup=BeautifulSoup(p.content , 'html.parser')\n",
        "    for i in soup.findAll('div',{'class':'article__chapo'}):\n",
        "        act.append(i.text)\n",
        "    for i in soup.findAll('div',{'class':'article__body'}):\n",
        "        act.append(i.text)   \n",
        "\n",
        "test1=[]\n",
        "test2=[]\n",
        "for i in range(0,len(act),2):\n",
        "  m=i\n",
        "  test1.append(act[m])\n",
        "for i in range(1,len(act),2):\n",
        "  m=i\n",
        "  test2.append(act[m])\n",
        "articleo=[]\n",
        "for x,y in zip(test1,test2):\n",
        "  articleo.append(x+y)\n",
        "\n",
        "articleof=[]\n",
        "for i in articleo :\n",
        "  i = re.sub(r'\\[[0-9]*\\]', ' ', i)\n",
        "  i = re.sub(r'\\s+', ' ', i)\n",
        "  articleof.append(i.strip())\n",
        "do=ds[0:6]\n",
        "\n",
        "\n",
        "#Recuperation images des articles:\n",
        "imagesActualiteOise = [BeautifulSoup(requests.get(li).content, 'html.parser').find('div', {'class': 'article__main-media'}).find('img')['src'] for li in lif[0:6]]\n",
        "LiensActualiteOise = [li for li in lif[0:6]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzPA5--wmO2y"
      },
      "source": [
        "oise_tour=[]\n",
        "for i in articleof:\n",
        "  text = i\n",
        "  splitter = SentenceSplitter(language='fr')\n",
        "  sentence_list = splitter.split(text)\n",
        "\n",
        "  translated_sentences = [GoogleTranslator(target='en').translate(element) for element in sentence_list]\n",
        "  paraphrased_sentences = [get_response(e,1) for e in translated_sentences]\n",
        "  cleaned_paraphrased_sentences = [e[0] for e in paraphrased_sentences]\n",
        "  sentences_to_french = [GoogleTranslator(target='fr').translate(e) for e in cleaned_paraphrased_sentences]\n",
        "  paraphrased_result = [' '.join(x for x in sentences_to_french)]\n",
        "  paraphrased_result_cleaned = str(paraphrased_result).strip('[]').strip(\"'\").strip('\"')\n",
        "  oise_tour.append(paraphrased_result_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-TOYAw1kKVT"
      },
      "source": [
        "dict = {'dates': do,\n",
        "        'image actualite': imagesActualiteOise,\n",
        "        'acctualites' : oise_tour,\n",
        "        'liensActualite': LiensActualiteOise\n",
        "        } \n",
        "dfo = pd.DataFrame(dict)\n",
        "dfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49AlXaEnkKXo"
      },
      "source": [
        "oise_t=dfo.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH9zyvOH2vPq"
      },
      "source": [
        "### Département de Oise (coronavirus)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz2O49D6n-tz"
      },
      "source": [
        "\n",
        "req_Oise = requests.get('https://actu.fr/oise/societe/coronavirus')\n",
        "soup_Oise = BeautifulSoup(req_Oise.content , 'html.parser')\n",
        "links_Oise = []\n",
        "artPrevLarge_Oise = soup_Oise.find('article',class_='ac-preview-article ac-preview-article-large-v').find('a')['href']\n",
        "links_Oise.append(artPrevLarge_Oise)\n",
        "\n",
        "artPrevSmall_Oise = soup_Oise.findAll('li',class_='ac-preview-article ac-preview-article-small')\n",
        "for e in artPrevSmall_Oise:\n",
        "    links_Oise.append(e.find('a')['href'])\n",
        "\n",
        "artPrevMedium_Oise= soup_Oise.findAll('li',class_='ac-preview-article ac-preview-article-medium')\n",
        "for e in artPrevMedium_Oise:\n",
        "    links_Oise.append(e.find('a')['href'])\n",
        "\n",
        "Publish_Dates_Oise = []\n",
        "Articles_Content_Oise = []\n",
        "lieno=links_Oise[0:6]\n",
        "\n",
        "for link in lieno:\n",
        "    r=requests.get(link)\n",
        "    soup_Oise=BeautifulSoup(r.content , 'html.parser')\n",
        "    t = soup_Oise.find('div', class_='ac-article-content')\n",
        "\n",
        "    children = t.findChildren(\"p\" , recursive=False)\n",
        "    art_content_Oise = \"\"\n",
        "    for e in children:\n",
        "        if e.name == 'p':\n",
        "            art_content_Oise += e.text\n",
        "    art_content_Oise = art_content_Oise.replace(u'\\xa0', u' ')\n",
        "\n",
        "    pub_date_Oise = soup_Oise.find('div', class_='ac-article-date').find('time')['datetime']\n",
        "    \n",
        "    Publish_Dates_Oise.append(pub_date_Oise)\n",
        "    Articles_Content_Oise.append(art_content_Oise)\n",
        "\n",
        "\n",
        "date_oise = [datetime.datetime.fromisoformat(x).strftime(\"%B %d, %Y at %H:%M\") for x in Publish_Dates_Oise]\n",
        "\n",
        "#Recuperation images des articles:\n",
        "imagesActualiteOiseCor = [BeautifulSoup(requests.get(li).content, 'html.parser').find('article', {'class': 'js-article-inner'}).find('img')['src'] for li in lieno]\n",
        "linksActualiteOiseCor = [li for li in lieno]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXTekYoyXjpK"
      },
      "source": [
        "oise_articles=Articles_Content_Oise[0:6]\n",
        "oise_cor=[]\n",
        "for i in oise_articles:\n",
        "  text = i\n",
        "  splitter = SentenceSplitter(language='fr')\n",
        "  sentence_list = splitter.split(text)\n",
        "\n",
        "  translated_sentences = [GoogleTranslator(target='en').translate(element) for element in sentence_list]\n",
        "  paraphrased_sentences = [get_response(e,1) for e in translated_sentences]\n",
        "  cleaned_paraphrased_sentences = [e[0] for e in paraphrased_sentences]\n",
        "  sentences_to_french = [GoogleTranslator(target='fr').translate(e) for e in cleaned_paraphrased_sentences]\n",
        "  paraphrased_result = [' '.join(x for x in sentences_to_french)]\n",
        "  paraphrased_result_cleaned = str(paraphrased_result).strip('[]').strip(\"'\").strip('\"')\n",
        "  oise_cor.append(paraphrased_result_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t_7Gzj0Xjxe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co1JvnZen-wL"
      },
      "source": [
        "Articles_Oise = {'dates': date_oise,\n",
        "                 'image article': imagesActualiteOiseCor,\n",
        "                 'actualite' : oise_cor,\n",
        "                 'lienActualite': linksActualiteOiseCor\n",
        "        } \n",
        "Oise_df = pd.DataFrame(Articles_Oise)\n",
        "Oise_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se_Bkx4Cn-3d"
      },
      "source": [
        "oise_c=Oise_df.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWr2WRWDYs5D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUOzaiXnZEQK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l5EMO99ZESP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4_5GpbDZEXq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRoEO5qO2RFR"
      },
      "source": [
        "### Département de Marne"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocpAgSCPkKZi"
      },
      "source": [
        "  # Recuperation des liens des articles\n",
        "p=requests.get('http://www.marne.fr/')\n",
        "soup=BeautifulSoup(p.content , 'html.parser')\n",
        "lim=[]\n",
        "d=[]\n",
        "for i in soup.findAll('div',{'class':'slide-data'}):\n",
        "    dm=i.find('div',{'class':'date'})\n",
        "    l=i.find('a')\n",
        "    d.append(dm.text)\n",
        "    lim.append(l['href'])\n",
        "\n",
        "datem=d[0:6]\n",
        "liensm=lim[0:6]\n",
        "articlesm=[]\n",
        "for link in liensm:\n",
        "    p=requests.get(link)\n",
        "    soup=BeautifulSoup(p.content , 'html.parser')\n",
        "    for i in soup.findAll('div',{'class':'field field-name-body field-type-text-with-summary apply-editor-styles'}):\n",
        "      articlesm.append(i.text)\n",
        "articlemf=[]\n",
        "for i in articlesm :\n",
        "    i = re.sub(r'\\[[0-9]*\\]', ' ', i)\n",
        "    i = re.sub(r'\\s+', ' ', i)\n",
        "    articlemf.append(i)\n",
        "\n",
        "#Recuperation images des articles:\n",
        "imagesActualiteMarne = [BeautifulSoup(requests.get(li).content, 'html.parser').find('div', {'class': 'field field-name-field-visuel field-type-image'}).find('img')['src'] for li in liensm]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw3xxAqHkKbj"
      },
      "source": [
        "marne_tour=[]\n",
        "for i in articlemf:\n",
        "  text = i\n",
        "  splitter = SentenceSplitter(language='fr')\n",
        "  sentence_list = splitter.split(text)\n",
        "\n",
        "  translated_sentences = [GoogleTranslator(target='en').translate(element) for element in sentence_list]\n",
        "  paraphrased_sentences = [get_response(e,1) for e in translated_sentences]\n",
        "  cleaned_paraphrased_sentences = [e[0] for e in paraphrased_sentences]\n",
        "  sentences_to_french = [GoogleTranslator(target='fr').translate(e) for e in cleaned_paraphrased_sentences]\n",
        "  paraphrased_result = [' '.join(x for x in sentences_to_french)]\n",
        "  paraphrased_result_cleaned = str(paraphrased_result).strip('[]').strip(\"'\").strip('\"')\n",
        "  marne_tour.append(paraphrased_result_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65LGvppPisCw"
      },
      "source": [
        "date_marne=[ r.strip(' ').strip().split()[2] for r in datem ]\n",
        "\n",
        "dict = {'dates': date_marne,\n",
        "        'image actualite': imagesActualiteMarne,\n",
        "        'acctualites' : marne_tour[:6]\n",
        "        } \n",
        "dfm = pd.DataFrame(dict)\n",
        "dfm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll8CrXRcdsDJ"
      },
      "source": [
        "marne_t=dfm.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNJEdlqi1KiW"
      },
      "source": [
        "### département de Marne (coronavirus)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "071Hg2RQd47r"
      },
      "source": [
        "  # Recuperation des liens des articles\n",
        "req_Marne = requests.get('https://actu.fr/la-marne/societe/coronavirus')\n",
        "soup_Marne = BeautifulSoup(req_Marne.content , 'html.parser')\n",
        "\n",
        "links_Marne = []\n",
        "artPrevLarge_Marne = soup_Marne.find('article',class_='ac-preview-article ac-preview-article-large-v').find('a')['href']\n",
        "links_Marne.append(artPrevLarge_Marne)\n",
        "\n",
        "artPrevSmall_Marne = soup_Marne.findAll('li',class_='ac-preview-article ac-preview-article-small')\n",
        "for e in artPrevSmall_Marne:\n",
        "    links_Marne.append(e.find('a')['href'])\n",
        "\n",
        "artPrevMedium_Marne= soup_Marne.findAll('li',class_='ac-preview-article ac-preview-article-medium')\n",
        "for e in artPrevMedium_Marne:\n",
        "    links_Marne.append(e.find('a')['href'])\n",
        "\n",
        "  # Recuperation des iimages et contenus des articles\n",
        "Publish_Dates_Marne = []\n",
        "Articles_Content_Marne = []\n",
        "\n",
        "for link in links_Marne[0:6]:\n",
        "    r=requests.get(link)\n",
        "    soup_Marne=BeautifulSoup(r.content , 'html.parser')\n",
        "    t = soup_Marne.find('div', class_='ac-article-content')\n",
        "\n",
        "    children = t.findChildren(\"p\" , recursive=False)\n",
        "    art_content_Marne = \"\"\n",
        "    for e in children:\n",
        "        if e.name == 'p':\n",
        "            art_content_Marne += e.text\n",
        "    art_content_Marne = art_content_Marne.replace(u'\\xa0', u' ')\n",
        "\n",
        "    pub_date_Marne = soup_Marne.find('div', class_='ac-article-date').find('time')['datetime']\n",
        "    \n",
        "    Publish_Dates_Marne.append(pub_date_Marne)\n",
        "    Articles_Content_Marne.append(art_content_Marne)\n",
        "\n",
        "\n",
        "#Recuperation images des articles:\n",
        "imagesActualiteMarneCor = [BeautifulSoup(requests.get(li).content, 'html.parser').find('article', {'class': 'js-article-inner'}).find('img')['src'] for li in links_Marne[0:6]]\n",
        "linksActualiteMarneCor = [li for li in links_Marne[0:6]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TEUdRz1IRv2"
      },
      "source": [
        "imagesActualiteMarneCor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn3byHROgeM5"
      },
      "source": [
        "marne_cor=[]\n",
        "for i in Articles_Content_Marne:\n",
        "  text = i\n",
        "  splitter = SentenceSplitter(language='fr')\n",
        "  sentence_list = splitter.split(text)\n",
        "\n",
        "  translated_sentences = [GoogleTranslator(target='en').translate(element) for element in sentence_list]\n",
        "  paraphrased_sentences = [get_response(e,1) for e in translated_sentences]\n",
        "  cleaned_paraphrased_sentences = [e[0] for e in paraphrased_sentences]\n",
        "  sentences_to_french = [GoogleTranslator(target='fr').translate(e) for e in cleaned_paraphrased_sentences]\n",
        "  paraphrased_result = [' '.join(x for x in sentences_to_french)]\n",
        "  paraphrased_result_cleaned = str(paraphrased_result).strip('[]').strip(\"'\").strip('\"')\n",
        "  marne_cor.append(paraphrased_result_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdtHNiasd496"
      },
      "source": [
        "date_m = [datetime.datetime.fromisoformat(x).strftime(\"%B %d, %Y at %H:%M\") for x in Publish_Dates_Marne]\n",
        "\n",
        "Articles_Marne = {'dates': date_m,\n",
        "        'imageActualite': imagesActualiteMarneCor,\n",
        "        'actualite' : marne_cor,\n",
        "        'lienActualite': linksActualiteMarneCor\n",
        "        } \n",
        "Marne_df = pd.DataFrame(Articles_Marne)\n",
        "Marne_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDUhREcKdsMR"
      },
      "source": [
        "marne_c=Marne_df.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vChsjqPAiV5R"
      },
      "source": [
        "acctualites_touristiques = [paris_t,seine_t,yveline_t,oise_t,marne_t]\n",
        "actualites_sanitaires= [paris_c,seine_c,yveline_c,oise_c,marne_c]\n",
        "dict = {'actualites_touristiques': acctualites_touristiques,\n",
        "        'actualites_sanitaires': actualites_sanitaires \n",
        "        } \n",
        "    \n",
        "df = pd.DataFrame(dict,index =['paris', 'seine_marne','yveline','oise','marne'])\n",
        "    \n",
        "df \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHylglpImvzW"
      },
      "source": [
        "with open('Acctualite_touristiques_sanitaires.json', 'w', encoding='utf-8') as file:\n",
        "    df.to_json(file, force_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfGop3Nymyqn"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('Acctualite_touristiques_sanitaires.json')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}